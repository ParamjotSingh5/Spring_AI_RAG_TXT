
## Installation and Running Locally
### Download [ollama](https://docs.ollama.com/) locally: 
```
brew install ollama
# Start the ollama service, this will run in the background:
brew services start ollama
```
### and pull a model LLM models, for embedding and generation tasks:
```
ollama pull mxbai-embed-large
ollama pull llama3
```

### Run application locally:
```
cd <project-root-directory>`
./gradlew bootRun
```

### Access the application:
Open your HTTP client and make a POST request to `http://localhost:8080/query_document` with a JSON body containing your question. For example:
```json
{
  "question": "What different resource types are in kubernetes."
}
```

The application will respond with an answer generated by the LLM based on the content of the `resource_type_in_kubernetes.md`.

We can expect a plain/text response similar to:
```
According to the documents, there are different types of resources in Kubernetes, including:

* Namespace: Provides a way to divide resources in a cluster.
* Pod: Smallest unit of K8s. Pods are an abstraction over containers.

Additionally, there are built-in resources such as:

* LimitRange
* NetworkPolicy
* MutatingWebhookConfiguration
* ValidatingWebhookConfiguration
* HorizontalPodAutoscaler

These resource types can be represented in an RBAC role using a slash (`/`) to delimit the resource and subresource.
```

### Note:
#### This question on "kubernetes resource types" can be answered by any LLM model, why do we need RAG then? 
The LLM can generate answers based on its training data, but it may not have access to the most up-to-date or user-specific information contained in the provided documents.
RAG allows the model to retrieve relevant information from the documents, ensuring that the answers are accurate and contextually relevant to the specific content provided.

For example, we can feed our resume documents to the RAG system and ask specific about relevance to a job description. 

## Request lifecycle
On application startup, the application reads the content of the `resource_type_in_kubernetes.md` file and generates embeddings using the `mxbai-embed-large` model from Ollama. These embeddings are stored in an [in-memory vector store](https://docs.spring.io/spring-ai/docs/current/api/org/springframework/ai/vectorstore/SimpleVectorStore.html) for efficient retrieval.

When a user submits a question via the `/query_document` endpoint, the application generates an embedding for the question using the same embedding model. It then retrieves the most relevant document chunks from the vector store based on cosine similarity.

The retrieved document chunks are combined with the user's question to form a prompt, which is then sent to the `llama3` model for answer generation. The model processes the prompt and generates a response based on the context provided by the retrieved documents.

Finally, the generated answer is returned to the user as a plain text response.